# -*- coding: utf-8 -*-
"""Marketing Bancario-Proyecto Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u8QAR_9VixbCwhikN4sUao_CnyNKb06P

# **PROYECTO FINAL**

# **1. Descripción.**

Se utilizara una metodologia de aprendizaje supervisado siguiendo los siguientes pasos:

**i) Comprensión del Problema:** Entender el objetivo y requisitos del proyecto

**ii)Recopilación de Datos:** Obtener el conjunto de datos necesario.

**iii)Exploración y Preparación de Datos:** Limpiar y explorar los datos.

#**I. Comprension del Problema**

Los datos están relacionados con campañas de marketing directo de una institución bancaria portuguesa.


Las campañas de marketing se basaron en llamadas telefónicas (telefono, celular).

🔹 Objetivo: Predecir si un cliente aceptara un producto (tarjeta, préstamo).

#**II. Recopilacion de Datos**
"""

import pandas as pd

# leer datasets
train_set = pd.read_csv('/content/bank-additional/bank-additional-full.csv',sep=';')
test_set = pd.read_csv('/content/bank-additional/bank-additional.csv',sep=';')

# ver registros nulos
def validarNulos(df):
  valoresNulos = df.isnull().sum()
  porcentajeNulos = ( valoresNulos /len(df)) *100
  dfNulos = pd.DataFrame({'Valores Nulos' : valoresNulos, 'Procentaje Nulos': porcentajeNulos})

  display(dfNulos)

validarNulos(train_set)

train_set.info()

train_set.head()

#columnas categoricas
col_categoricas = list(train_set.select_dtypes(include='object').columns)
#columnas numericas
col_numericas = list(train_set.select_dtypes(exclude='object').columns)

import matplotlib.pyplot as plt
import seaborn as sns

fig,axes = plt.subplots(4,3,figsize=(15,20))
axes = axes.flatten()
for idx,col_cat in enumerate(col_categoricas):
  if idx < len(axes):
    sns.countplot(x=col_cat, data=train_set, hue='y', ax=axes[idx] )
    axes[idx].set_title(f'Distribución de {col_cat} por Subscripción')
    axes[idx].set_xlabel('')
    axes[idx].tick_params(axis='x', rotation=45)
    axes[idx].legend(title='Subscribió?', loc='upper right')

for idx in range(len(col_categoricas), len(axes)):
    fig.delaxes(axes[idx])

plt.tight_layout()
plt.show()

fig_num, axes_num = plt.subplots(4, 3, figsize=(15,20))
axes_num = axes_num.flatten()

for idx, col_num in enumerate(col_numericas):
    sns.boxplot(x='y',hue="y", y=col_num, data=train_set, ax=axes_num[idx])
    axes_num[idx].set_title(f'Distribución de {col_num} por Subscripción', fontsize=12)
    axes_num[idx].set_xlabel('Subscribió?', fontsize=10)
    axes_num[idx].set_ylabel(col_num, fontsize=10)

for idx in range(len(col_numericas), len(axes_num)):
    fig_num.delaxes(axes_num[idx])

plt.tight_layout()
plt.show()

cols_outlayers = ['age', 'duration', 'campaign','previous']
for col in cols_outlayers:
    Q1 = train_set[col].quantile(0.25)
    Q3 = train_set[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = train_set[(train_set[col] < (Q1 - 1.5 * IQR)) | (train_set[col] > (Q3 + 1.5 * IQR))]
    print(f"Outliers en {col}: {len(outliers)} ({len(outliers)/len(train_set)*100:.2f}%), valor superior: {Q3 + 1.5 * IQR}")

def normalize_month(df):
  lst_mes = {"ene": 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, 'jul': 7, 'aug': 8, 'sep': 9, 'oct':10, 'nov':11, 'dec':12}
  df["month"] = df["month"].map(lst_mes).astype(type(1))

def normalize_days(df):
  lst_days = {"mon": 1, "tue": 2, "wed": 3, "thu": 4, "fri": 5, "sat": 6, "sun": 7}
  df["day_of_week"] = df["day_of_week"].map(lst_days).astype(type(1))

def group_education(df):
  df["education"] = df["education"].replace(regex=[r'^basic.*$', 'illiterate', "unknown"], value='educacion_baja') \
                    .replace(regex=["high.school"], value='educacion_media') \
                    .replace(regex=['university.degree', 'professional.course'], value='educacion_alta')

def group_job(df):
  df["job"] = df["job"].replace(to_replace =["entrepreneur","blue-collar","self-employed", "technician", "services"], value="salario_variable") \
                       .replace(to_replace =["retired","housemaid","admin.","management"], value="salario_fijo") \
                       .replace(to_replace =["unemployed","student","unknown"], value="sin_salario_fijo")

def agrupar_deuda(df):
  lst_deuda = ["housing", "loan"]
  procesar = set(lst_deuda) - set(df.columns)

  def cal_value(row):
    if 'yes' in [ row['housing'], row['loan']]:
      return 1
    elif 'unknown' in [row['housing'], row['loan']]:
      return 2
    else:
      return 0

  if len(procesar) == 0:
    df['deuda'] = df.apply(cal_value, axis=1)
    df.drop(["housing", "loan"], axis=1, inplace=True)

def convertir_categoricas_numericas(df,cols):
  df.drop("contact",axis=1, inplace=True)
  df = pd.get_dummies(df, columns=cols)

  cols_bool = list(df.select_dtypes(include='bool').columns)

  for colBool in cols_bool:
    df[colBool] = df[colBool].astype(int)
  return df

from sklearn.preprocessing import LabelEncoder
def label_enconding(df):
  label_encoder = LabelEncoder()

  for column in df.columns:
    if not pd.api.types.is_numeric_dtype(df[column]):
      df[column] = label_encoder.fit_transform(df[column].astype(str))

  return df

#Tratamiento de Datos Entrenamiento
normalize_month(train_set)
normalize_days(train_set)
group_education(train_set)
group_job(train_set)
agrupar_deuda(train_set)

cols_convertir = ["marital","job","education"]
train_set = convertir_categoricas_numericas(train_set,cols_convertir)
train_set = label_enconding(train_set)

train_set.columns

train_set.head()

figure, axes = plt.subplots( 3 , 4, figsize=(18, 10))

for column, axe in zip(train_set, axes.flatten()):
    ax = sns.histplot(train_set[column], ax=axe, kde=True)
    ax.set_title(f'Distribución de {column}')
    ax.set_xlabel('')
plt.tight_layout()

def create_bins(df, column, bins_dict):
    bins_list = bins_dict.get("bins_list")
    bins_number = bins_dict.get("bins_number")

    if bins_list:
        data_for_bins = pd.cut(df[column], bins=bins_list, precision=0, duplicates="drop")
    else:
        data_for_bins = df[column].copy()

        min_value = data_for_bins.min()
        max_value = data_for_bins.max()

        temp_values = data_for_bins.tolist()

        if len(data_for_bins[data_for_bins == min_value]) > 1:
            temp_values.append(min_value - 1)

        if len(data_for_bins[data_for_bins == max_value]) > 1:
            temp_values.append(max_value + 1)

        quantiles = pd.qcut(pd.Series(temp_values), q=bins_number, precision=0, duplicates="drop")

        if len(temp_values) > len(data_for_bins):
            quantiles = quantiles[:-1]

        data_for_bins = quantiles

    dictionary_of_intervals = {interval: idx for idx, interval in enumerate(data_for_bins.cat.categories)}

    bins_column = "Bins_for_" + column
    df[bins_column + "_index"] = data_for_bins.cat.rename_categories(dictionary_of_intervals)

    df[bins_column + "_index"] = df[bins_column + "_index"].astype('category')

    print(f"Bins for {column} creados")
    return df

bins_dict = {
    "age": {"bins_number": 0, "bins_list": [0,20,30,40,50,60,70,100]},
	  "duration": {"bins_number": 5, "bins_list": []},
	  "campaign": {"bins_number": 4, "bins_list": []},
	  "emp.var.rate": {"bins_number": 5, "bins_list": []},
	  "cons.price.idx": {"bins_number": 5, "bins_list": []},
	  "cons.conf.idx": {"bins_number": 5, "bins_list": []},
	  "euribor3m": {"bins_number": 5, "bins_list": []}
}

for column, bins_list in bins_dict.items():
  create_bins(train_set, column, bins_list)

train_set.head()

from sklearn.preprocessing import StandardScaler

corr = train_set.corr()
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

from sklearn.model_selection import train_test_split,cross_val_score, KFold, StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier



"""#**III. Elección del mejor modelo**"""

#train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)

#col_saved = train_set.columns

#scaler = StandardScaler()
#train_set_scaled = scaler.fit_transform(train_set.drop(labels=['y'], axis=1))

#train_set_scaled = pd.DataFrame(train_set_scaled)
#train_set_scaled.columns = col_saved[:-1]
#train_set_scaled

#x = train_set_scaled
x = train_set.drop(labels=['y'], axis=1)
y = train_set['y']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)

models = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC()
}

k = 5
kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

for name, model in models.items():
    scores = cross_val_score(model, x, y, cv=kf, scoring='accuracy')
    print(f"{name}: Accuracy promedio = {scores.mean():.4f} (+/- {scores.std():.4f})")

warnings.filterwarnings("default", category=UserWarning)

"""#**IV. Modelo DecisionTreeClassifier**"""

tree_model = DecisionTreeClassifier(criterion='entropy',random_state=42)
tree_model.fit(x_train, y_train)

tree_model.score(x_train, y_train) , tree_model.score(x_test, y_test)

max_depth_range = list(range(1, 20))

accuracy = []
for depth in max_depth_range:

    clf = DecisionTreeClassifier(criterion='entropy', max_depth = depth,random_state=42)
    clf.fit(x_train, y_train)

    score_train  = clf.score(x_train, y_train)
    score_test  = clf.score(x_test, y_test)
    accuracy.append(score_test)

    print('depth=',depth, ' con score train=',score_train, ' y score test=',score_test)

import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, roc_curve, auc,classification_report,roc_auc_score,f1_score

best_depth = max_depth_range[np.argmax(accuracy)]
f'Mejor valor accuracy en la capa {best_depth}'

plt.plot(max_depth_range, accuracy, label='Test Accuracy', marker='o')
plt.axvline(x=best_depth, color='red', linestyle='--', label=f'Mejor profundidad (depth={best_depth})')
plt.xlabel('Profundidad del Árbol')
plt.ylabel('Accuracy')
plt.title('Distribución de profundidad')
plt.legend()
plt.grid()
plt.show()

final_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=best_depth,random_state=42)
final_tree_model.fit(x_train, y_train)

final_tree_model.score(x_train, y_train) , final_tree_model.score(x_test, y_test)

import graphviz
from sklearn import tree

tree_clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth = best_depth,random_state=42)
tree_clf.fit(x_train,y_train)
dot_data = tree.export_graphviz(tree_clf,feature_names = x.columns.tolist())
graph = graphviz.Source(dot_data)

graph

y_pred_first_model = tree_model.predict(x_test)
y_pred = final_tree_model.predict(x_test)

def validacion_modelo(model,x_train,y_train,y_test,y_pred):
  acc_tree = accuracy_score(y_test,y_pred)
  f1_tree = f1_score(y_test,y_pred)
  val_mean_f1_tree = cross_val_score(model,x_train,y_train,cv=5,scoring='f1_macro').mean()
  val_mean_acc_tree = cross_val_score(model,x_train,y_train,cv=5,scoring='accuracy').mean()
  test_mse= mean_squared_error(y_test,y_pred)


  print(f"accuracy score del modelo: {acc_tree:.4f} ({acc_tree * 100:.2f}%)")
  print(f"f1_score del modelo: {f1_tree:.4f} ({f1_tree * 100:.2f}%)")
  print(f"Promedio de validación cruzada f1 macro {val_mean_f1_tree} ({val_mean_f1_tree * 100:.2f}%)")
  print(f"Promedio de validación cruzada accuracy {val_mean_acc_tree} ({val_mean_acc_tree * 100:.2f}%)")
  print(f"Error de Prediccion en datos de Testeo MSE: {test_mse} ({test_mse * 100:.2f}%)")

def grafico_roc_auc(model,x_test,y_test):
  y_pred_proba = model.predict_proba(x_test)[::,1]

  fpr, tpr, th = roc_curve(y_test,  y_pred_proba)
  auc = roc_auc_score(y_test, y_pred_proba)
  plt.title("Curva ROC y AUC")
  plt.plot(fpr,tpr,label="AUC="+str(auc))
  plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
  plt.xlabel("FPR")
  plt.ylabel("TPR")
  plt.legend(loc=4)
  plt.show()

def matrix_confusion_grid(y_test,y_pred):
  print("Confusion Matrix")
  display(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))

def matrix_confusion_graf(model,y_test,y_pred,best):
  cm = confusion_matrix(y_test, y_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot(cmap='Blues')
  plt.title(f"Matriz de Confusión (Arbol de Decisión - best_depth={best})")
  plt.show()

validacion_modelo(final_tree_model,x_train,y_train,y_test,y_pred)

grafico_roc_auc(final_tree_model,x_test,y_test)

matrix_confusion_grid(y_test,y_pred)

matrix_confusion_graf(final_tree_model,y_test,y_pred,best_depth)

"""#**V. Probando con otro Dataset el Modelo DecisionTreeClassifier**"""

# Tratamiento de Datos Prueba (otro data set con datos diferentes)
normalize_month(test_set)
normalize_days(test_set)
group_education(test_set)
group_job(test_set)
agrupar_deuda(test_set)
test_set = convertir_categoricas_numericas(test_set,cols_convertir)
test_set = label_enconding(test_set)

for column, bins_list in bins_dict.items():
  create_bins(test_set, column, bins_list)

# Datos de prueba
test_x = test_set.drop(labels=['y'], axis=1)
test_y = test_set['y']

# prediccion de los datos de prueba
y_pred_test = final_tree_model.predict(test_x)

# Validación de la predicción de los datos de prueba
validacion_modelo(final_tree_model,x_train,y_train,test_y,y_pred_test)

# gráfico de la curva roc de los datos de prueba
grafico_roc_auc(final_tree_model,test_x,test_y)

# matriz confusion de los datos de prueba, con los valores reales de los datos de prueba
matrix_confusion_grid(test_y,y_pred_test)

# matriz confusion de los datos de prueba, con los valores reales de los datos de prueba
matrix_confusion_graf(final_tree_model,test_y,y_pred_test,best_depth)

"""#**V. Modelo RandomForestClassifier**"""

modelRandom = RandomForestClassifier(criterion='entropy', random_state=42)
modelRandom.fit(x_train, y_train)

modelRandom.score(x_train, y_train) , modelRandom.score(x_test, y_test)

train_accuracy = []
test_accuracy = []

for depth in max_depth_range:
    rf = RandomForestClassifier(criterion='entropy', max_depth=depth, random_state=42)
    rf.fit(x_train, y_train)

    train_acc = rf.score(x_train, y_train)
    test_acc = rf.score(x_test, y_test)

    train_accuracy.append(train_acc)
    test_accuracy.append(test_acc)

    print(f'depth={depth}: Train Accuracy={train_acc:.4f}, Test Accuracy={test_acc:.4f}')

best_depth_random = max_depth_range[np.argmax(test_accuracy)]
f'Mejor valor accuracy en la capa {best_depth_random}'

plt.figure(figsize=(10, 6))
plt.plot(max_depth_range, test_accuracy, label='Test Accuracy', marker='o')
plt.axvline(x=best_depth_random, color='red', linestyle='--', label=f'Mejor profundidad (depth={best_depth_random})')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('RandomForest: Accuracy vs. Profundidad de los Árboles')
plt.legend()
plt.grid()
plt.show()

final_random_model = RandomForestClassifier(criterion='entropy', max_depth=best_depth_random, random_state=42)
final_random_model.fit(x_train, y_train)

final_random_model.score(x_train, y_train) , final_random_model.score(x_test, y_test)

tree_index = 0
single_tree = final_random_model.estimators_[tree_index]

dot_data = tree.export_graphviz(single_tree,
                                feature_names=x_train.columns.tolist(),
                                filled=True, rounded=True, special_characters=True)

graph = graphviz.Source(dot_data)
graph

y_pred_random = final_random_model.predict(x_test)

validacion_modelo(final_random_model,x_train,y_train,y_test,y_pred_random)

grafico_roc_auc(final_random_model,x_test,y_test)

matrix_confusion_grid(y_test,y_pred_random)

matrix_confusion_graf(final_random_model,y_test,y_pred_random,best_depth_random)

"""#**V. Probando con otro Dataset el Modelo RandomForestClassifier**"""

y_pred_random_test = final_random_model.predict(test_x)

validacion_modelo(final_random_model,x_train,y_train,test_y,y_pred_random_test)

grafico_roc_auc(final_random_model,test_x,test_y)

matrix_confusion_grid(test_y,y_pred_random_test)

matrix_confusion_graf(final_random_model,test_y,y_pred_random_test,best_depth_random)

import pickle
pickle.dump(final_random_model,open("ModeloRandomForestBank.md","wb"))